    <!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<meta name="author" content="Cameron Roach">
		<meta name="description" content="A data science blog">
		<meta name="generator" content="Hugo 0.21" />
		<title>GEFCom2017 tutorial: part 1 &middot; Cameron&#39;s blog</title>
		<link rel="shortcut icon" href="/images/favicon.ico">
		<link rel="stylesheet" href="/css/style.css">
		<link rel="stylesheet" href="/css/highlight.css">
		<link rel="stylesheet" href="/css/zenburn.css">
		<script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
		

		
		<link rel="stylesheet" href="/css/font-awesome.min.css">
		

		
		<link href="/index.xml" rel="alternate" type="application/rss+xml" title="Cameron&#39;s blog" />
		
	</head>

    <body>
       <nav class="main-nav">
	
	
		<a href='/'> <span class="arrow">←</span>Home</a>
	
	<a href='/post'>Archive</a>
	<a href='/tags'>Tags</a>
	<a href='/about'>About</a>

	

	
	<a class="cta" href="/index.xml">Subscribe</a>
	
</nav>


        <section id="wrapper">
            <article class="post">
                <header>
                    <h1>
                        GEFCom2017 tutorial: part 1
                    </h1>
                    <h2 class="headline">
                    Sep 28, 2018 14:35
                    · 2056 words
                    · 10 minutes read
                      <span class="tags">
                      
                      
                          
                              <a href="/tags/r">R</a>
                          
                              <a href="/tags/tutorial">Tutorial</a>
                          
                              <a href="/tags/forecasting">Forecasting</a>
                          
                              <a href="/tags/electricity">Electricity</a>
                          
                      
                      
                      </span>
                    </h2>
                </header>
                
                <section id="post-body">
                    <div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>This post shows how to implement the model I propose in the paper <em>Reconciled boosted models for GEFCom2017 hierarchical probabilistic load forecasting</em> <span class="citation">(Roach 2018)</span>.</p>
<p>Before starting, make sure you have the following packages installed:</p>
<ul>
<li><code>tidyverse</code></li>
<li><code>lubridate</code></li>
<li><code>caret</code></li>
<li><code>gefcom2017data</code>.</li>
</ul>
<p>To install the <code>gefcom2017</code> package run:</p>
<pre class="r"><code>install.packages(&quot;devtools&quot;)
devtools::install_github(&quot;camroach87/gefcom2017data&quot;)</code></pre>
<p>We will also be using the <code>xgboost</code> package, but <code>caret</code> should automatically install this when we fit models.</p>
<div id="data" class="section level3">
<h3>Data</h3>
<p>The <code>gefcom</code> data frame contains the data we wish to work with. Eight bottom-level zones and two aggregated zones are included.</p>
<pre class="r"><code>library(caret)
library(tidyverse)
library(lubridate)
library(gefcom2017data)

gefcom
## # A tibble: 1,241,710 x 15
## # Groups:   zone [10]
##    ts                  zone  demand drybulb dewpnt date        year month
##    &lt;dttm&gt;              &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt; &lt;fct&gt;
##  1 2017-01-01 00:00:00 NEMA…  2421.      40     37 2017-01-01  2017 Jan  
##  2 2017-01-01 01:00:00 NEMA…  2335.      39     36 2017-01-01  2017 Jan  
##  3 2017-01-01 02:00:00 NEMA…  2263.      38     35 2017-01-01  2017 Jan  
##  4 2017-01-01 03:00:00 NEMA…  2217.      37     35 2017-01-01  2017 Jan  
##  5 2017-01-01 04:00:00 NEMA…  2206.      37     35 2017-01-01  2017 Jan  
##  6 2017-01-01 05:00:00 NEMA…  2231.      37     35 2017-01-01  2017 Jan  
##  7 2017-01-01 06:00:00 NEMA…  2291.      37     34 2017-01-01  2017 Jan  
##  8 2017-01-01 07:00:00 NEMA…  2331.      36     33 2017-01-01  2017 Jan  
##  9 2017-01-01 08:00:00 NEMA…  2379.      38     32 2017-01-01  2017 Jan  
## 10 2017-01-01 09:00:00 NEMA…  2429.      40     31 2017-01-01  2017 Jan  
## # … with 1,241,700 more rows, and 7 more variables: hour &lt;dbl&gt;,
## #   day_of_week &lt;fct&gt;, day_of_year &lt;dbl&gt;, weekend &lt;lgl&gt;,
## #   holiday_name &lt;chr&gt;, holiday &lt;lgl&gt;, trend &lt;dbl&gt;</code></pre>
<p>A week of electricity demand data for each zone is shown below.</p>
<pre class="r"><code>gefcom %&gt;%
  filter(ts &gt;= dmy(&quot;1/2/2017&quot;),
         ts &lt; dmy(&quot;8/2/2017&quot;)) %&gt;%
  ggplot(aes(x = ts, y = demand, colour = zone)) +
  geom_line() +
  theme(legend.position = &quot;bottom&quot;)</code></pre>
<p><img src="/post/2018-09-28-gefcom2017-tut-1_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>We will nest the data frame by zone to make it easier to work with. This will make storing models and results easier.</p>
<pre class="r"><code>gefcom &lt;- gefcom %&gt;%
  group_by(zone) %&gt;%
  nest()

gefcom
## # A tibble: 10 x 2
##    zone       data                   
##    &lt;chr&gt;      &lt;list&gt;                 
##  1 NEMASSBOST &lt;tibble [124,171 × 14]&gt;
##  2 WCMASS     &lt;tibble [124,171 × 14]&gt;
##  3 SEMASS     &lt;tibble [124,171 × 14]&gt;
##  4 RI         &lt;tibble [124,171 × 14]&gt;
##  5 CT         &lt;tibble [124,171 × 14]&gt;
##  6 VT         &lt;tibble [124,171 × 14]&gt;
##  7 NH         &lt;tibble [124,171 × 14]&gt;
##  8 ME         &lt;tibble [124,171 × 14]&gt;
##  9 MASS       &lt;tibble [124,171 × 14]&gt;
## 10 TOTAL      &lt;tibble [124,171 × 14]&gt;</code></pre>
</div>
<div id="preprocessing" class="section level3">
<h3>Preprocessing</h3>
<p>Electricity demand is typically affected by weather conditions on previous days. To obtain better fitting models we include lags for the last 72 periods. We can use the <code>add_lags</code> function to do this. You can examine the new structure of the data for one of the zones below.</p>
<pre class="r"><code>add_lags &lt;- function(data, variables, lags = 1) {
  for (iV in variables) {
    for (iL in lags) {
      lag_name &lt;- paste0(iV, &quot;_lag&quot;, iL)
      data[[lag_name]] &lt;- lag(data[[iV]], iL)
    }
  }
  
  # Remove NAs in early rows due to lags
  data &lt;- data %&gt;% 
    slice((max(lags)+1):nrow(data))
  
  data
}

gefcom &lt;- gefcom %&gt;%
  mutate(data = map(data, add_lags, variables = c(&quot;drybulb&quot;, &quot;dewpnt&quot;), 
                    lags = 1:72))

gefcom %&gt;%
  filter(zone == &quot;SEMASS&quot;) %&gt;%
  unnest()
## # A tibble: 124,099 x 159
##    zone  ts                  demand drybulb dewpnt date        year month
##    &lt;chr&gt; &lt;dttm&gt;               &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt; &lt;fct&gt;
##  1 SEMA… 2017-01-04 00:00:00  1279.      45     43 2017-01-04  2017 Jan  
##  2 SEMA… 2017-01-04 01:00:00  1223.      44     43 2017-01-04  2017 Jan  
##  3 SEMA… 2017-01-04 02:00:00  1190.      44     42 2017-01-04  2017 Jan  
##  4 SEMA… 2017-01-04 03:00:00  1192.      43     43 2017-01-04  2017 Jan  
##  5 SEMA… 2017-01-04 04:00:00  1236.      43     40 2017-01-04  2017 Jan  
##  6 SEMA… 2017-01-04 05:00:00  1362.      43     39 2017-01-04  2017 Jan  
##  7 SEMA… 2017-01-04 06:00:00  1573.      41     39 2017-01-04  2017 Jan  
##  8 SEMA… 2017-01-04 07:00:00  1692.      41     39 2017-01-04  2017 Jan  
##  9 SEMA… 2017-01-04 08:00:00  1709.      41     38 2017-01-04  2017 Jan  
## 10 SEMA… 2017-01-04 09:00:00  1711.      41     38 2017-01-04  2017 Jan  
## # … with 124,089 more rows, and 151 more variables: hour &lt;dbl&gt;,
## #   day_of_week &lt;fct&gt;, day_of_year &lt;dbl&gt;, weekend &lt;lgl&gt;,
## #   holiday_name &lt;chr&gt;, holiday &lt;lgl&gt;, trend &lt;dbl&gt;, drybulb_lag1 &lt;dbl&gt;,
## #   drybulb_lag2 &lt;dbl&gt;, drybulb_lag3 &lt;dbl&gt;, drybulb_lag4 &lt;dbl&gt;,
## #   drybulb_lag5 &lt;dbl&gt;, drybulb_lag6 &lt;dbl&gt;, drybulb_lag7 &lt;dbl&gt;,
## #   drybulb_lag8 &lt;dbl&gt;, drybulb_lag9 &lt;dbl&gt;, drybulb_lag10 &lt;dbl&gt;,
## #   drybulb_lag11 &lt;dbl&gt;, drybulb_lag12 &lt;dbl&gt;, drybulb_lag13 &lt;dbl&gt;,
## #   drybulb_lag14 &lt;dbl&gt;, drybulb_lag15 &lt;dbl&gt;, drybulb_lag16 &lt;dbl&gt;,
## #   drybulb_lag17 &lt;dbl&gt;, drybulb_lag18 &lt;dbl&gt;, drybulb_lag19 &lt;dbl&gt;,
## #   drybulb_lag20 &lt;dbl&gt;, drybulb_lag21 &lt;dbl&gt;, drybulb_lag22 &lt;dbl&gt;,
## #   drybulb_lag23 &lt;dbl&gt;, drybulb_lag24 &lt;dbl&gt;, drybulb_lag25 &lt;dbl&gt;,
## #   drybulb_lag26 &lt;dbl&gt;, drybulb_lag27 &lt;dbl&gt;, drybulb_lag28 &lt;dbl&gt;,
## #   drybulb_lag29 &lt;dbl&gt;, drybulb_lag30 &lt;dbl&gt;, drybulb_lag31 &lt;dbl&gt;,
## #   drybulb_lag32 &lt;dbl&gt;, drybulb_lag33 &lt;dbl&gt;, drybulb_lag34 &lt;dbl&gt;,
## #   drybulb_lag35 &lt;dbl&gt;, drybulb_lag36 &lt;dbl&gt;, drybulb_lag37 &lt;dbl&gt;,
## #   drybulb_lag38 &lt;dbl&gt;, drybulb_lag39 &lt;dbl&gt;, drybulb_lag40 &lt;dbl&gt;,
## #   drybulb_lag41 &lt;dbl&gt;, drybulb_lag42 &lt;dbl&gt;, drybulb_lag43 &lt;dbl&gt;,
## #   drybulb_lag44 &lt;dbl&gt;, drybulb_lag45 &lt;dbl&gt;, drybulb_lag46 &lt;dbl&gt;,
## #   drybulb_lag47 &lt;dbl&gt;, drybulb_lag48 &lt;dbl&gt;, drybulb_lag49 &lt;dbl&gt;,
## #   drybulb_lag50 &lt;dbl&gt;, drybulb_lag51 &lt;dbl&gt;, drybulb_lag52 &lt;dbl&gt;,
## #   drybulb_lag53 &lt;dbl&gt;, drybulb_lag54 &lt;dbl&gt;, drybulb_lag55 &lt;dbl&gt;,
## #   drybulb_lag56 &lt;dbl&gt;, drybulb_lag57 &lt;dbl&gt;, drybulb_lag58 &lt;dbl&gt;,
## #   drybulb_lag59 &lt;dbl&gt;, drybulb_lag60 &lt;dbl&gt;, drybulb_lag61 &lt;dbl&gt;,
## #   drybulb_lag62 &lt;dbl&gt;, drybulb_lag63 &lt;dbl&gt;, drybulb_lag64 &lt;dbl&gt;,
## #   drybulb_lag65 &lt;dbl&gt;, drybulb_lag66 &lt;dbl&gt;, drybulb_lag67 &lt;dbl&gt;,
## #   drybulb_lag68 &lt;dbl&gt;, drybulb_lag69 &lt;dbl&gt;, drybulb_lag70 &lt;dbl&gt;,
## #   drybulb_lag71 &lt;dbl&gt;, drybulb_lag72 &lt;dbl&gt;, dewpnt_lag1 &lt;dbl&gt;,
## #   dewpnt_lag2 &lt;dbl&gt;, dewpnt_lag3 &lt;dbl&gt;, dewpnt_lag4 &lt;dbl&gt;,
## #   dewpnt_lag5 &lt;dbl&gt;, dewpnt_lag6 &lt;dbl&gt;, dewpnt_lag7 &lt;dbl&gt;,
## #   dewpnt_lag8 &lt;dbl&gt;, dewpnt_lag9 &lt;dbl&gt;, dewpnt_lag10 &lt;dbl&gt;,
## #   dewpnt_lag11 &lt;dbl&gt;, dewpnt_lag12 &lt;dbl&gt;, dewpnt_lag13 &lt;dbl&gt;,
## #   dewpnt_lag14 &lt;dbl&gt;, dewpnt_lag15 &lt;dbl&gt;, dewpnt_lag16 &lt;dbl&gt;,
## #   dewpnt_lag17 &lt;dbl&gt;, dewpnt_lag18 &lt;dbl&gt;, dewpnt_lag19 &lt;dbl&gt;,
## #   dewpnt_lag20 &lt;dbl&gt;, dewpnt_lag21 &lt;dbl&gt;, …</code></pre>
<p><a href="https://www.youtube.com/watch?v=R2vBZuLI3oI">That’s a lot of lags!</a> Thankfully we can perform automatic feature selection and regularisation using the <code>caret</code> and <code>XGBoost</code> packages.</p>
<p>As a final step we will set the <code>holiday</code> variable to <code>FALSE</code> on those occasions a holiday falls on a weekend. It’s pretty clear from the boxplots below that while demand does drop a lot during weekday holidays, the same effect is not observed on weekends. Electricity demand seems to be about the same on weekends regardless of whether it is a public holiday or not.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<pre class="r"><code>gefcom %&gt;% 
  unnest() %&gt;% 
  ggplot(aes(x = weekend, y = demand, fill = holiday)) + 
  geom_boxplot(outlier.shape = &quot;o&quot;, outlier.alpha = 0.5) +
  facet_wrap(~zone, scales = &quot;free_y&quot;) +
  theme(legend.position = &quot;bottom&quot;)</code></pre>
<p><img src="/post/2018-09-28-gefcom2017-tut-1_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<pre class="r"><code>
gefcom &lt;- gefcom %&gt;% 
  unnest() %&gt;% 
  mutate(holiday = if_else(weekend==TRUE, FALSE, holiday)) %&gt;% 
  group_by(zone) %&gt;%
  nest()</code></pre>
</div>
</div>
<div id="fitting-models" class="section level2">
<h2>Fitting models</h2>
<p>Here we’ll use <code>caret</code> to train and carry out cross-validation for our models.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> You can do your own cross-validation by changing the <code>method</code> argument in <code>trainControl</code> and updating the list that defines <code>xgb_grid</code> to include extra hyperparameter values. The <code>cross_df</code> function returns a data frame with all parameter combinations, so it is an easy way to create a grid for parameter tuning.</p>
<p>Note that we have set <code>allowParallel</code> to FALSE in <code>trainControl</code>. This is to avoid conflicts with XGBoost which automatically does parallel processing when the <code>nthread</code> argument is not set. I prefer using XGBoost’s parallelisation rather than <code>caret</code>’s as I found myself running into RAM usage issues with <code>caret</code>.</p>
<p>I’ve also created a function to fit our benchmark model, Tao’s Vanilla energy model <span class="citation">(Hong 2010)</span>. It has been used in several GEFComs (including GEFCom2017) and is the go-to energy forecasting benchmark model due to its ubiquity and ease of use. Note that we do not do any cross-validation or feature selection for this model - we use it as is.</p>
<pre class="r"><code>xgb_ctrl &lt;- trainControl(
  method = &quot;repeatedcv&quot;,
  number = 5,
  allowParallel = FALSE,
  returnData = FALSE,
  trim = TRUE,
  returnResamp = &quot;none&quot;,
  savePredictions = &quot;none&quot;
)

xgb_grid &lt;- list(
  nrounds = 50,
  alpha = c(0, 50),
  lambda = c(0, 50),
  eta = 0.1
) %&gt;%
  cross_df() %&gt;% 
  as.data.frame()  # caret throws a weird error without

fit_boosted_model &lt;- function(data, zone, ...) {
  cat(&quot;Carrying out cross-validation for zone&quot;, zone, &quot;...\n&quot;)
  
  data %&gt;%
    select(demand, hour, day_of_year, day_of_week, holiday, trend,
           starts_with(&quot;drybulb&quot;), starts_with(&quot;dewpnt&quot;)) %&gt;%
    na.omit() %&gt;%
    sample_frac(...) %&gt;% 
    train(demand ~ .,
          data = .,
          method=&quot;xgbLinear&quot;,
          trControl = xgb_ctrl,
          tuneGrid = xgb_grid)
}

fit_vanilla_model &lt;- function(data, zone, ...) {
  cat(&quot;Fitting vanilla model for zone&quot;, zone, &quot;...\n&quot;)
  
  data %&gt;%
    select(demand, hour, month, day_of_week, trend, drybulb) %&gt;%
    na.omit() %&gt;%
    sample_frac(...) %&gt;% 
    train(demand ~ day_of_week*hour + poly(drybulb, 3, raw = TRUE)*month +
            poly(drybulb, 3, raw = TRUE)*hour + trend,
          data = .,
          method = &quot;lm&quot;,
          trControl = xgb_ctrl)
}</code></pre>
<p>Before fitting our models we need to split data into training and test data sets. I’m using a month of data for our test set and all remaining historical data for training. Note that I’ve added a <code>frac</code> parameter to subsample the training data.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> This is purely to reduce the computation time when I compile this blog. You can remove this or set it to another value depending on how you want to compromise between speed and performance.</p>
<pre class="r"><code># Create train and test data sets
gefcom &lt;- gefcom %&gt;% 
  mutate(train = map(data, function(x) filter(x, x$date &lt; dmy(&quot;1/4/2017&quot;))),
         test = map(data, function(x) filter(x, x$date &gt;= dmy(&quot;1/4/2017&quot;)))) %&gt;% 
  select(-data)

# Train models
gefcom &lt;- gefcom %&gt;%
  mutate(caret_xgb = map2(train, zone, fit_boosted_model, size = 0.01),
         caret_vanilla = map2(train, zone, fit_vanilla_model, size = 0.01))
## Carrying out cross-validation for zone NEMASSBOST ...
## Carrying out cross-validation for zone WCMASS ...
## Carrying out cross-validation for zone SEMASS ...
## Carrying out cross-validation for zone RI ...
## Carrying out cross-validation for zone CT ...
## Carrying out cross-validation for zone VT ...
## Carrying out cross-validation for zone NH ...
## Carrying out cross-validation for zone ME ...
## Carrying out cross-validation for zone MASS ...
## Carrying out cross-validation for zone TOTAL ...
## Fitting vanilla model for zone NEMASSBOST ...
## Fitting vanilla model for zone WCMASS ...
## Fitting vanilla model for zone SEMASS ...
## Fitting vanilla model for zone RI ...
## Fitting vanilla model for zone CT ...
## Fitting vanilla model for zone VT ...
## Fitting vanilla model for zone NH ...
## Fitting vanilla model for zone ME ...
## Fitting vanilla model for zone MASS ...
## Fitting vanilla model for zone TOTAL ...</code></pre>
<div id="best-fit" class="section level3">
<h3>Best fit</h3>
<p>Ok great, we’ve done our cross-validation (using 1% of the available data :-O) and now it’s time to pick the best model. Once again, <code>purrr</code>’s <code>map</code> function comes to the fore.</p>
<pre class="r"><code>gefcom &lt;- gefcom %&gt;% 
  mutate(final_xgb = map(caret_xgb, &quot;finalModel&quot;),
         final_vanilla = map(caret_vanilla, &quot;finalModel&quot;))</code></pre>
<p>Our data frame now contains data, cross-validation results and the final models for each zone.</p>
<pre class="r"><code>gefcom
## # A tibble: 10 x 7
##    zone   train    test    caret_xgb caret_vanilla final_xgb  final_vanilla
##    &lt;chr&gt;  &lt;list&gt;   &lt;list&gt;  &lt;list&gt;    &lt;list&gt;        &lt;list&gt;     &lt;list&gt;       
##  1 NEMAS… &lt;tibble… &lt;tibbl… &lt;S3: tra… &lt;S3: train&gt;   &lt;S3: xgb.… &lt;S3: lm&gt;     
##  2 WCMASS &lt;tibble… &lt;tibbl… &lt;S3: tra… &lt;S3: train&gt;   &lt;S3: xgb.… &lt;S3: lm&gt;     
##  3 SEMASS &lt;tibble… &lt;tibbl… &lt;S3: tra… &lt;S3: train&gt;   &lt;S3: xgb.… &lt;S3: lm&gt;     
##  4 RI     &lt;tibble… &lt;tibbl… &lt;S3: tra… &lt;S3: train&gt;   &lt;S3: xgb.… &lt;S3: lm&gt;     
##  5 CT     &lt;tibble… &lt;tibbl… &lt;S3: tra… &lt;S3: train&gt;   &lt;S3: xgb.… &lt;S3: lm&gt;     
##  6 VT     &lt;tibble… &lt;tibbl… &lt;S3: tra… &lt;S3: train&gt;   &lt;S3: xgb.… &lt;S3: lm&gt;     
##  7 NH     &lt;tibble… &lt;tibbl… &lt;S3: tra… &lt;S3: train&gt;   &lt;S3: xgb.… &lt;S3: lm&gt;     
##  8 ME     &lt;tibble… &lt;tibbl… &lt;S3: tra… &lt;S3: train&gt;   &lt;S3: xgb.… &lt;S3: lm&gt;     
##  9 MASS   &lt;tibble… &lt;tibbl… &lt;S3: tra… &lt;S3: train&gt;   &lt;S3: xgb.… &lt;S3: lm&gt;     
## 10 TOTAL  &lt;tibble… &lt;tibbl… &lt;S3: tra… &lt;S3: train&gt;   &lt;S3: xgb.… &lt;S3: lm&gt;</code></pre>
</div>
</div>
<div id="next-time" class="section level2">
<h2>Next time</h2>
<p>In the next post we will perform weather and residual simulations before using hierarchical reconciliation to create cohesive probabilistic forecasts.</p>
</div>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references">
<div id="ref-Hong2010-cy">
<p>Hong, Tao. 2010. “Short Term Electric Load Forecasting.” PhD thesis, North Carolina State University.</p>
</div>
<div id="ref-Roach2018-pf">
<p>Roach, Cameron. 2018. “Reconciled Boosted Models for GEFCom2017 Hierarchical Probabilistic Load Forecasting.” <em>International Journal of Forecasting</em>. <a href="http://dx.doi.org/10.1016/j.ijforecast.2018.09.009" class="uri">http://dx.doi.org/10.1016/j.ijforecast.2018.09.009</a>.</p>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Note that the wider whiskers and extra outliers for non-holidays is a reflection of having much more data for those days. There are few public holidays falling on weekends relative to non-holidays.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>I’m only doing 5-fold cross-validation. I suspect you’d get slightly better results carrying out time series cross-validation.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>You will also notice that I’m creating different training sets for our boosted and vanilla models. It’s pretty trivial to make these the same but I don’t want to bog this tutorial down in minor details so I’ll leave this as an exercise! In practice you’ll want to use all the data so it’s a moot point.<a href="#fnref3">↩</a></p></li>
</ol>
</div>

                </section>
            </article>

            

            
<section id="comments">
  <div id="disqus_thread"></div>
  <script>
  var disqus_config = function () {
  
  };
  (function() {
    var inIFrame = function() {
      var iframe = true;
      try { iframe = window.self !== window.top; } catch (e) {}
      return iframe;
    };
    if (inIFrame()) return;
    var d = document, s = d.createElement('script');
    s.src = '//camroach87-github-io.disqus.com/embed.js'; s.async = true;
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>



            

            <footer id="footer">
    
        <div id="social">

	
	
    <a class="symbol" href="https://github.com/camroach87">
        <i class="fa fa-github-square"></i>
    </a>
    
    <a class="symbol" href="https://www.linkedin.com/in/cameron-roach-00873b69/">
        <i class="fa fa-linkedin-square"></i>
    </a>
    
    <a class="symbol" href="https://twitter.com/camroach87">
        <i class="fa fa-twitter-square"></i>
    </a>
    


</div>

    
    <p class="small">
    
       © Copyright 2019 <i class="fa fa-heart" aria-hidden="true"></i> Cameron Roach
    
    </p>
    <p class="small">
        Powered by <a href="http://www.gohugo.io/">Hugo</a> Theme By <a href="https://github.com/nodejh/hugo-theme-cactus-plus">nodejh</a>
    </p>
</footer>

        </section>

        <script src="/js/jquery-2.2.4.min.js"></script>
<script src="/js/main.js"></script>




  
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-108233353-1', 'auto');
ga('send', 'pageview');
</script>





    </body>
</html>
